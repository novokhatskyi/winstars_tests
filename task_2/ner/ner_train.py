"""LSTM (Long Short-Term Memory)
Різновид рекурентної мережі (RNN), яка «пам’ятає» контекст послідовності слів довше завдяки коміркам пам’яті. 
Добре підходить для задач, де порядок слів важливий (текст, мова).

CRF (Conditional Random Field)
Ймовірнісна надбудова над токенами: замість того, щоб передбачати тег кожного слова незалежно, 
CRF враховує сусідні теги, щоб послідовність була узгодженою (наприклад, “B-ANIMAL” не може безпосередньо переходити в “I-PERSON”). 
В NER це часто дає чистіші, цілісніші підсвічені сутності.

LSTM + CRF разом
Типовий класичний стек для NER:
токени → ембедінги → BiLSTM (контекст) → CRF (узгоджені послідовності тегів).
Плюс: простіше, легше тренувати на CPU, мало залежностей. Мінус: гірше контекстні зв’язки на рівні фраз/довгих залежностей, 
ніж сучасні трансформери.

BERT (base)
Трансформер, який читає фразу двонапрямно (ліворуч+праворуч) і дає контекстні ембедінги для кожного токена. 
«base» — це стандартний розмір (≈110М параметрів). Для NER зазвичай беруть bert-base-cased/-uncased або україно-/мульти-мовні варіанти, 
і додають зверху тонкий класифікатор токенів.
Плюс: зазвичай точніший за LSTM+CRF, краще розуміє контекст. Мінус: важчий, повільніший.

Висновок для цього проєкту:
щоб рухатись швидко й стабільно в середовищі, робимо спрощений варіант NER (легкий трансформер або навіть BiLSTM без CRF). 
Головна ціль — витягнути назви тварин з тексту."""